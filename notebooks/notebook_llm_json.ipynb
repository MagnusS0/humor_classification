{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "#llm = Llama.from_pretrained(repo_id=\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\", filename= \"*Q4_K_M.gguf\", \n",
    "                            #n_gpu_layers = -1,\n",
    "                            #n_ctx=512,\n",
    "                            #max_tokens=12,\n",
    "                            #n_batch=512,\n",
    "                            #n_threads=6,\n",
    "                            #verbose=True,\n",
    "                            #flash_attn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spin up server using `llama-cpp-python[server]` package.\n",
    "```bash\t\n",
    "python -m llama_cpp.server --hf_model_repo_id NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF --model '*Q4_K_M.gguf' --n_ctx 512 --n_batch 256 --n_threads 6 --flash_attn True --chat_format chatml --n_gpu_layers 30  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"rating\": 4\\n}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\", # \"http://<Your api-server IP>:port\"\n",
    "    api_key = \"sk-no-key-required\"\n",
    ")\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 1,\n",
    "            \"maximum\": 5,\n",
    "            \"description\": \"The rating of the joke, from 1 to 5.\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "joke = \"Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    "\n",
    "def rate_joke(joke: str):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a joke evaluator that answers in JSON. Here's the json schema you must adhere to:\\n<schema>\\n{schema}\\n</schema>\",\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\" Your task is to evaluate jokes based on their funniness on a scale from 1 to 5, \n",
    "                            where 1 represents the least funny and 5 represents the most funny. \n",
    "                            Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"{joke}\" \"\"\"},\n",
    "    ],\n",
    "    #response_format= {\"type\": \"json_object\"}, #\"schema\": schema}, # uncomment this line to enforce the schema\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    max_tokens=10,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "rate_joke(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pretraining_data = pd.read_parquet('../data/processed/pretraining_data.parquet')\n",
    "\n",
    "# take a subset of 100 jokes to test\n",
    "pretraining_data = pretraining_data.sample(100)\n",
    "\n",
    "pretraining_data['rating'] = pretraining_data['cleaned_joke'].apply(rate_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "3    57\n",
       "2    29\n",
       "4    11\n",
       "1     3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretraining_data['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the ratings to the original ratings {\"rating\": 3}  keep only the number\n",
    "pretraining_data['rating'] = pretraining_data['rating'].str.extract('(\\d+)')\n",
    "pretraining_data['rating'] = pretraining_data['rating'].astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(pretraining_data['rating'], pretraining_data['score_class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
