{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/ready_for_model.csv', index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.score_class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the \"cleaned_joke\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the require NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Custom Stopword list\n",
    "stop_words = [\n",
    "    \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \n",
    "    \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \n",
    "    \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \n",
    "    \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \n",
    "    \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \n",
    "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"very\", \"s\", \"t\", \"can\", \n",
    "    \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "]\n",
    "\n",
    "# Define the stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define the stemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "# Define the lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# Define the function to clean the text\n",
    "def preprocessing(text, how = 'lem'):\n",
    "\n",
    "    # Remove the stopwords and apply lemmitization/stemming\n",
    "    if how == 'lem':\n",
    "        cleaned_text = [lem.lemmatize(word) for word in word_tokenize(text.lower()) if (word not in stop_words) & (word.isalnum())]\n",
    "    else:\n",
    "        cleaned_text = [stem.stem(word) for word in word_tokenize(text.lower()) if (word not in stop_words) & (word.isalnum())]\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function\n",
    "df['joke_new'] = df['joke_new'].apply(lambda x: preprocessing(x, 'lem'))\n",
    "\n",
    "# Verify the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Word2Vec embeddings for the \"cleaned_joke\" column to predict the \"score_class\", i.e., label of a joke\n",
    "\n",
    "\t1.\tTrain a Word2Vec model on your jokes dataset. --> potentially increase number of features\n",
    "\t2.\tGenerate sentence vectors for each joke.\n",
    "\t3.\tTrain a Naive Bayes classifier using the sentence vectors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare data for Word2Vec\n",
    "sentences = df['joke_new']\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=50, window=10, min_count=1, workers=4, epochs=20)\n",
    "\n",
    "# Save the model for future use\n",
    "word2vec_model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sentence vectors\n",
    "def avg_feature_vector(words, model, num_features = 50):\n",
    "    feature_vec = np.zeros(num_features, dtype='float32')\n",
    "    n_words = 0\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[word])\n",
    "        except KeyError:\n",
    "            # Token not in model\n",
    "            pass\n",
    "    return (feature_vec / n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the vectorize_joke function to each cleaned joke\n",
    "df['joke_vector'] = df['joke_new'].apply(lambda x: avg_feature_vector(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the result\n",
    "df['joke_vector'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the joke_vector column\n",
    "nan_count = df['joke_vector'].apply(lambda x: np.isnan(x).any()).sum()\n",
    "print(f\"Number of NaN values in joke_vector: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the NaN values with the mean of the joke_vector column\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Convert the joke vectors to a 2D array\n",
    "X = np.array(df['joke_vector'].tolist())\n",
    "\n",
    "# Impute NaN values with the mean of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare the data for training\n",
    "y = df['score_class']\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and Train the Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate accuracy of the model and create classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model: {accuracy}\")\n",
    "\n",
    "# Generate the classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)\n",
    "\n",
    "# Accuracy with vector size 100: 0.23065\n",
    "# Accuracy with vector size 300: 0.23077\n",
    "# Accuracy with vector size 200: 0.229546\n",
    "# Accuracy with vector size 100 (GridSearch Parameters): 0.235141\n",
    "# Accuracy with vector size 50: 0.236414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare data for Word2Vec\n",
    "sentences = df['joke_new']\n",
    "\n",
    "# Function to train Word2Vec and evaluate embeddings\n",
    "def train_and_evaluate(params):\n",
    "    model = Word2Vec(sentences, vector_size=params['vector_size'], window=params['window'], \n",
    "                     min_count=params['min_count'], workers=4, epochs=params['epochs'])\n",
    "    \n",
    "    # Generate sentence vectors\n",
    "    def vectorize_joke(joke, model):\n",
    "        vectors = [model.wv[word] for word in joke if word in model.wv]\n",
    "        if len(vectors) == 0:\n",
    "            return np.zeros(model.vector_size)\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    df['joke_vector'] = df['joke_new'].apply(lambda x: vectorize_joke(x, model))\n",
    "    X = np.array(df['joke_vector'].tolist())\n",
    "\n",
    "    # Impute NaN values with the mean of the column\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # Prepare the target variable\n",
    "    y = df['score_class']\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the Gaussian Naive Bayes classifier\n",
    "    nb_classifier = GaussianNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the test data\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'vector_size': [100, 200],\n",
    "    'window': [10, 15, 20],\n",
    "    'min_count': [1],\n",
    "    'epochs': [10, 20]\n",
    "}\n",
    "\n",
    "# Initialize best parameters and best accuracy\n",
    "best_params = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Perform grid search\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "    accuracy = train_and_evaluate(params)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
