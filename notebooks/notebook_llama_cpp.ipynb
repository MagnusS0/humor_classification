{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models locally and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use llm = Llama(model_id = path_to_model) to load a model from a local path\n",
    "\n",
    "llm = Llama.from_pretrained(repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\", filename= \"*q4.gguf\", \n",
    "                            n_gpu_layers = -1,\n",
    "                            n_ctx=512,\n",
    "                            max_tokens=20,\n",
    "                            n_batch=512,\n",
    "                            n_threads=16,\n",
    "                            verbose=True,\n",
    "                            flash_attn=True,\n",
    "                            chat_format='chatml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r/Jokes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke = \"Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0,\n",
    "            \"maximum\": 4,\n",
    "            \"description\": \"The rating of the joke, from 0 to 4.\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "llm.create_chat_completion(    \n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a joke evaluator that answers in JSON. Here's the json schema you must adhere to:\\n{schema}\",\n",
    "    },\n",
    "    {\"role\": \"user\",\n",
    "        \"content\": f\"\"\" Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, \n",
    "                        where 0 represents the least funny and 4 represents the most funny. \n",
    "                        \\n \"{joke}\" \"\"\"},\n",
    "],\n",
    "#response_format= {\"type\": \"json_object\"}, #\"schema\": schema}, # uncomment this line to enforce the schema slows down the completion\n",
    "temperature=0.2,\n",
    "top_p=0.9,\n",
    "max_tokens=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set it up as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke = \"Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0,\n",
    "            \"maximum\": 4,\n",
    "            \"description\": \"The rating of the joke, from 0 to 4.\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def rate_joke(joke: str):\n",
    "    completion = llm.create_chat_completion(    \n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a joke evaluator that answers in JSON. Here's the json schema you must adhere to:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\" Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny.\\n \"{joke}\" \"\"\"},\n",
    "    ],\n",
    "    #response_format= {\"type\": \"json_object\"}, #\"schema\": schema}, # uncomment this line to enforce the schema\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    max_tokens=10,\n",
    "    )\n",
    "\n",
    "    return (completion['choices'][0]['message']['content'])\n",
    "\n",
    "rate_joke(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"../data/interim/ready_for_model.csv\")\n",
    "data = data[['joke_new', 'score_class']]\n",
    "data = data.rename(columns = {'joke_new': 'text', 'score_class': 'label'})\n",
    "\n",
    "# Subsample to speed up testing\n",
    "data = data.sample(frac=0.25, random_state=42)\n",
    "\n",
    "# Make all columns objects\n",
    "data[\"text\"] = data[\"text\"].astype(\"object\")\n",
    "data[\"label\"] = data[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42, shuffle=True, stratify=data[\"label\"])\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42, stratify=test[\"label\"])\n",
    "\n",
    "test['rating'] = test['text'].apply(rate_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the rating from the response\n",
    "test['rating_n'] = test['rating'].str.extract('(\\d+)')\n",
    "\n",
    "# Drop rows where the rating is missing (e.g. the model did not return a rating)\n",
    "test = test.dropna(subset=['rating_n'])\n",
    "\n",
    "# Convert the rating to an integer\n",
    "test['rating'] = test['rating'].astype(int)\n",
    "\n",
    "\n",
    "accuracy_score(test['rating'], test['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test['label'], test['rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(test['label'], test['rating'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG News dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Fears for T N pension after talks\"\n",
    "description = \"Unions representing workers at Turner Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"topic\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"world\", \"sports\", \"business\", \"science/tech\"],\n",
    "            \"description\": \"The topic of the news article.\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def rate_news(title: str, description: str):\n",
    "    completion = llm.create_chat_completion(    \n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a classifying expert that answers in JSON. Here's the json schema you must adhere to:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\" Your task is to classify news articles into one of the following topics: world, sports, business, or science/tech. Title:\\n \"{title}\" \\n Description:\\n \"{description}\"\"\"},\n",
    "    ],\n",
    "    #response_format= {\"type\": \"json_object\"}, #\"schema\": schema}, # uncomment this line to enforce the schema\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    max_tokens=10,\n",
    "    )\n",
    "\n",
    "    return (completion['choices'][0]['message']['content'])\n",
    "\n",
    "rate_news(title, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/external/ag_news_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1, random_state=42) # Adjust the fraction to speed up testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of label to class\n",
    "label2class = {1: 'world', 2: 'sports', 3: 'business', 4: 'science/tech'}\n",
    "class2label = {v: k for k, v in label2class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate news articles with columns Title and\tDescription\n",
    "data['class'] = data.apply(lambda x: rate_news(x['Title'], x['Description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class_n']= data['class'].apply(lambda x: x.strip())\n",
    "data['class_n'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some commen formatting issues\n",
    "data['class_n'] = data['class_n'].apply(lambda x: x.replace(\"'\", \"\\\"\"))\n",
    "data['class_n'] =data['class_n'].apply(lambda x: x.rstrip('\"'))\n",
    "data['class_n'] = data['class_n'].apply(lambda x: x.rstrip('}'))\n",
    "data['class_n'] = data['class_n'].apply(lambda x: x + '}')\n",
    "data['class_n'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use if JSON is not formatted correctly\n",
    "# If one of the classes are in string add it to that class\n",
    "def clean (x):\n",
    "    if 'world' in x:\n",
    "        return 'world'\n",
    "    elif 'sports' in x:\n",
    "        return 'sports'\n",
    "    elif 'business' in x:\n",
    "        return 'business'\n",
    "    elif 'science/tech' in x:\n",
    "        return 'science/tech'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "data['class_n'] = data['class_n'].apply(lambda x: clean(x))\n",
    "data['class_n'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use if JSON is formatted correctly\n",
    "def get_class_text(json_string):\n",
    "    try:\n",
    "        return json.loads(json_string)['topic']\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "data['class_n'] = data['class_n'].apply(get_class_text)\n",
    "\n",
    "data['class_n'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the class to the label\n",
    "data['class_n'] = data['class_n'].map(class2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nan/Unknown classes\n",
    "data = data.dropna(subset=['class_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class Index'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(data['Class Index'], data['class_n']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
