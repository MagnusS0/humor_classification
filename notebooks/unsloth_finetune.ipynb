{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing by Unsloth\n",
    "\n",
    "Install packages to be able to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BitsAndBytesConfig, AutoConfig\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "max_seq_length = 512\n",
    "dtype = None \n",
    "load_in_4bit = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, #\n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = {},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our dataset\n",
    "import pandas as pd\n",
    "# Load the data\n",
    "data = pd.read_csv(\"../data/interim/ready_for_model.csv\")\n",
    "data = data[['joke_new', 'score_class']]\n",
    "data = data.rename(columns = {'joke_new': 'text', 'score_class': 'label'})\n",
    "\n",
    "# Subsample to test the code\n",
    "data = data.sample(frac=0.3, random_state=42)\n",
    "\n",
    "# Make all columns objects\n",
    "data[\"text\"] = data[\"text\"].astype(\"object\")\n",
    "data[\"label\"] = data[\"label\"].astype(int)\n",
    "\n",
    "# Set up schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0,\n",
    "            \"maximum\": 4,\n",
    "            \"description\": \"The rating of the joke, from 0 to 5.\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Make labels JSON format\n",
    "data[\"label\"] = data[\"label\"].apply(lambda x: f'{{\"rating\": {x}}}')\n",
    "\n",
    "# Set up prompt format\n",
    "data[\"conversations\"] = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a joke evaluator that answers in JSON. Here's the json schema you must adhere to:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\" Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"{joke}\" \"\"\"},\n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": f\"{label}\"\n",
    "        }\n",
    "    ] for joke, label in zip(data[\"text\"], data[\"label\"])\n",
    "]\n",
    "\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42, shuffle= True, stratify=data[\"label\"])\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42, stratify=test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "\n",
    "# Format the prompts\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "#val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 16,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 1,\n",
    "        max_grad_norm=0.3,\n",
    "        learning_rate = 2e-4,\n",
    "        bf16 = True,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"constant\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as GGUF for llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/lora_model_json_2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"lora_model_full\")\n",
    "#model.save_pretrained_gguf(\"Phi-3-mini-4k-instruct-humor-full-clf-gguf\", tokenizer, quantization_method = \"q4_k_m\") # Will download Llama.cpp if not installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n",
    "It's faster to do this in the llama.cpp notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"./models/lora_model_json_2\", \n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the test dataset\n",
    "from datasets import Dataset\n",
    "# Set up test prompt format\n",
    "test[\"conversations\"] = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a joke evaluator that answers in JSON. Here's the json schema you must adhere to:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\" Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"{joke}\" \"\"\"},\n",
    "    ] for joke in test[\"text\"]\n",
    "]\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "# Format for generations:\n",
    "def formatting_prompts_func_gen(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\") for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# Format the prompts\n",
    "test_dataset = test_dataset.map(formatting_prompts_func_gen, batched=True)\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "# Evaluate the model\n",
    "def evaluate (model, test_dataset): \n",
    "    preds = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs = torch.tensor(test_dataset[i][\"text\"]).to('cuda')\n",
    "        outputs = model.generate(inputs, max_length = 512)\n",
    "        preds.append(tokenizer.decode(outputs[0], skip_special_tokens = True))\n",
    "    return preds\n",
    "\n",
    "# Get the predictions\n",
    "# time the evaluation\n",
    "preds = evaluate(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract the predictions\n",
    "preds_clean = [re.search(r'<\\|im_start\\|>assistant\\n(.*)', pred).group(1) for pred in preds]\n",
    "\n",
    "print(test[\"label\"][0:5])\n",
    "print(preds_clean[0:5])\n",
    "# Compute the accuracy\n",
    "accuracy_score(test[\"label\"], preds_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_clean_df = pd.DataFrame(preds_clean, columns = [\"preds\"])\n",
    "\n",
    "preds_clean_df[\"preds\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in list\n",
    "from collections import Counter\n",
    "\n",
    "Counter(test_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    mapping = {0: \"Not funny at all\", 1: \"Not funny\", 2: \"Funny\", 3: \"Very funny\", 4: \"Hilarious\"}\n",
    "    reverse_mapping = {v: k for k, v in mapping.items()}  # Reverse mapping for confusion matrix\n",
    "\n",
    "    # Ensure y_true is string labels\n",
    "    if isinstance(y_true[0], (int, np.integer)):\n",
    "        map_func = np.vectorize(mapping.get)\n",
    "        y_true = map_func(y_true)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    y_true_num = np.vectorize(reverse_mapping.get)(y_true)  # Convert back to numerical labels\n",
    "    y_pred_num = np.vectorize(reverse_mapping.get)(y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_num, y_pred=y_pred_num, labels=list(mapping.keys()))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "evaluate(test_dataset[\"label\"], preds_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
