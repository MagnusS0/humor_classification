{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BitsAndBytesConfig, AutoConfig\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "max_seq_length = 512\n",
    "dtype = None \n",
    "load_in_4bit = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You passed in `unsloth/Phi-3-mini-4k-instruct` and `load_in_4bit = True`.\n",
      "We shall load `unsloth/Phi-3-mini-4k-instruct-bnb-4bit` for 4x faster loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 512\n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, #\n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = {},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 119,537,664 || all params: 3,940,617,216 || trainable%: 3.03347565743366\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|endoftext|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our dataset\n",
    "import pandas as pd\n",
    "# Load the data\n",
    "data = pd.read_parquet(\"./humor_classification/data/processed/pretraining_data.parquet\")\n",
    "\n",
    "# Subsample to test the code\n",
    "data = data.sample(frac=0.3, random_state=42)\n",
    "\n",
    "# Make all columns objects\n",
    "data[\"text\"] = data[\"text\"].astype(\"object\")\n",
    "data[\"label\"] = data[\"label\"].astype(int)\n",
    "\n",
    "# Set up schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0,\n",
    "            \"maximum\": 4,\n",
    "            \"description\": \"The rating of the joke, from 0 to 5.\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Make labels JSON format\n",
    "data[\"label\"] = data[\"label\"].apply(lambda x: f'{{\"rating\": {x}}}')\n",
    "\n",
    "# Set up prompt format\n",
    "data[\"conversations\"] = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a joke evaluator that answers in JSON. Here's the json schema you must adhere to:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\" Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"{joke}\" \"\"\"},\n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": f\"{label}\"\n",
    "        }\n",
    "    ] for joke, label in zip(data[\"text\"], data[\"label\"])\n",
    "]\n",
    "\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42, shuffle= True, stratify=data[\"label\"])\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42, stratify=test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09325f5e97ac49679615a0c5c7631af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "\n",
    "# Format the prompts\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "#val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}}<|im_end|>\\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"My wife is like an animal, she has her needs.... like ..her need to not have sex that often. EDIT: Sorry I thought of this today. We just had a kid and it\\'s been a while.\" <|im_end|>\\n<|im_start|>assistant\\n{\"rating\": 3}<|im_end|>\\n',\n",
       " 'label': '{\"rating\": 3}',\n",
       " 'conversations': [{'content': \"You are a joke evaluator that answers in JSON. Here's the json schema you must adhere to:\\n{'type': 'object', 'properties': {'rating': {'type': 'number', 'minimum': 0, 'maximum': 4, 'description': 'The rating of the joke, from 0 to 5.'}}}\",\n",
       "   'role': 'system'},\n",
       "  {'content': ' Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"My wife is like an animal, she has her needs.... like ..her need to not have sex that often. EDIT: Sorry I thought of this today. We just had a kid and it\\'s been a while.\" ',\n",
       "   'role': 'user'},\n",
       "  {'content': '{\"rating\": 3}', 'role': 'assistant'}],\n",
       " '__index_level_0__': 192950}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bb2e8a61e542f1b371bfb006207ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/11690 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 16,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 1,\n",
    "        max_grad_norm=0.3,\n",
    "        learning_rate = 2e-4,\n",
    "        bf16 = True,\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_32bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"constant\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 11,690 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 182\n",
      " \"-____-\"     Number of trainable parameters = 119,537,664\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='182' max='182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [182/182 45:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.849900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.945100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.518200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.413900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.447600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.600500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.499200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.495200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.447600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.477300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.453600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.435900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.482800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.443900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.458600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.482100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.514900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.440100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.519100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.490900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.538900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.495200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.418500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.481600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.484800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.515300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 42.44 out of 60.46 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 36.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting mistral model. Can use fast conversion = True.\n",
      "Unsloth: Extending Phi-3-mini-4k-instruct-humor-full-clf-gguf/tokenizer.model with added_tokens.json.\n",
      "Originally tokenizer.model is of size (32000).\n",
      "But we need to extend to sentencepiece vocab size (32011).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n",
      " \"-____-\"     In total, you will have to wait around 26 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at Phi-3-mini-4k-instruct-humor-full-clf-gguf into f16 GGUF format.\n",
      "The output location will be ./Phi-3-mini-4k-instruct-humor-full-clf-gguf-unsloth.F16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:convert:Loading model file Phi-3-mini-4k-instruct-humor-full-clf-gguf/model-00001-of-00002.safetensors\n",
      "INFO:convert:Loading model file Phi-3-mini-4k-instruct-humor-full-clf-gguf/model-00001-of-00002.safetensors\n",
      "INFO:convert:Loading model file Phi-3-mini-4k-instruct-humor-full-clf-gguf/model-00002-of-00002.safetensors\n",
      "INFO:convert:model parameters count : 3821079552 (4B)\n",
      "INFO:convert:params = Params(n_vocab=32064, n_embd=3072, n_layer=32, n_ctx=4096, n_ff=8192, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('Phi-3-mini-4k-instruct-humor-full-clf-gguf'))\n",
      "INFO:convert:Loaded vocab file PosixPath('Phi-3-mini-4k-instruct-humor-full-clf-gguf/tokenizer.model'), type 'spm'\n",
      "INFO:convert:Vocab info: <SentencePieceVocab with 32011 base tokens and 0 added tokens>\n",
      "INFO:convert:Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 32000, 'unk': 0, 'pad': 32009}, add special tokens {'bos': True, 'eos': False}>\n",
      "INFO:convert:Writing Phi-3-mini-4k-instruct-humor-full-clf-gguf-unsloth.F16.gguf, format 1\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 32000\n",
      "INFO:gguf.vocab:Setting special token type unk to 0\n",
      "INFO:gguf.vocab:Setting special token type pad to 32009\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "INFO:convert:[  1/291] Writing tensor token_embd.weight                      | size  32064 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   1\n",
      "INFO:convert:[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[  5/291] Writing tensor blk.0.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[  7/291] Writing tensor blk.0.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[  8/291] Writing tensor blk.0.attn_output.weight               | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[  9/291] Writing tensor blk.0.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   1\n",
      "INFO:convert:[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 17/291] Writing tensor blk.1.attn_output.weight               | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 20/291] Writing tensor blk.10.attn_norm.weight                | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 21/291] Writing tensor blk.10.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   1\n",
      "INFO:convert:[ 22/291] Writing tensor blk.10.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 23/291] Writing tensor blk.10.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 24/291] Writing tensor blk.10.ffn_norm.weight                 | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 25/291] Writing tensor blk.10.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 26/291] Writing tensor blk.10.attn_output.weight              | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 27/291] Writing tensor blk.10.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 28/291] Writing tensor blk.10.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 29/291] Writing tensor blk.11.attn_norm.weight                | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 30/291] Writing tensor blk.11.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   1\n",
      "INFO:convert:[ 31/291] Writing tensor blk.11.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 32/291] Writing tensor blk.11.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 33/291] Writing tensor blk.11.ffn_norm.weight                 | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 34/291] Writing tensor blk.11.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 35/291] Writing tensor blk.11.attn_output.weight              | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 36/291] Writing tensor blk.11.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 37/291] Writing tensor blk.11.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 38/291] Writing tensor blk.12.attn_norm.weight                | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 39/291] Writing tensor blk.12.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   1\n",
      "INFO:convert:[ 40/291] Writing tensor blk.12.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 41/291] Writing tensor blk.12.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 42/291] Writing tensor blk.12.ffn_norm.weight                 | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 43/291] Writing tensor blk.12.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 44/291] Writing tensor blk.12.attn_output.weight              | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 45/291] Writing tensor blk.12.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 46/291] Writing tensor blk.12.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 47/291] Writing tensor blk.13.attn_norm.weight                | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 48/291] Writing tensor blk.13.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   1\n",
      "INFO:convert:[ 49/291] Writing tensor blk.13.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 50/291] Writing tensor blk.13.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 51/291] Writing tensor blk.13.ffn_norm.weight                 | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 52/291] Writing tensor blk.13.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 53/291] Writing tensor blk.13.attn_output.weight              | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 54/291] Writing tensor blk.13.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 55/291] Writing tensor blk.13.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 56/291] Writing tensor blk.14.attn_norm.weight                | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 57/291] Writing tensor blk.14.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   1\n",
      "INFO:convert:[ 58/291] Writing tensor blk.14.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 59/291] Writing tensor blk.14.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 60/291] Writing tensor blk.14.ffn_norm.weight                 | size   3072           | type F32  | T+   1\n",
      "INFO:convert:[ 61/291] Writing tensor blk.14.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 62/291] Writing tensor blk.14.attn_output.weight              | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 63/291] Writing tensor blk.14.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 64/291] Writing tensor blk.14.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   1\n",
      "INFO:convert:[ 65/291] Writing tensor blk.15.attn_norm.weight                | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[ 66/291] Writing tensor blk.15.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   2\n",
      "INFO:convert:[ 67/291] Writing tensor blk.15.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 68/291] Writing tensor blk.15.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 69/291] Writing tensor blk.15.ffn_norm.weight                 | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[ 70/291] Writing tensor blk.15.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 71/291] Writing tensor blk.15.attn_output.weight              | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 72/291] Writing tensor blk.15.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 73/291] Writing tensor blk.15.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 74/291] Writing tensor blk.16.attn_norm.weight                | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[ 75/291] Writing tensor blk.16.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   2\n",
      "INFO:convert:[ 76/291] Writing tensor blk.16.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 77/291] Writing tensor blk.16.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 78/291] Writing tensor blk.16.ffn_norm.weight                 | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[ 79/291] Writing tensor blk.16.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 80/291] Writing tensor blk.16.attn_output.weight              | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 81/291] Writing tensor blk.16.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 82/291] Writing tensor blk.16.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 83/291] Writing tensor blk.17.attn_norm.weight                | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[ 84/291] Writing tensor blk.17.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   2\n",
      "INFO:convert:[ 85/291] Writing tensor blk.17.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 86/291] Writing tensor blk.17.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 87/291] Writing tensor blk.17.ffn_norm.weight                 | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[ 88/291] Writing tensor blk.17.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 89/291] Writing tensor blk.17.attn_output.weight              | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 90/291] Writing tensor blk.17.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 91/291] Writing tensor blk.17.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 92/291] Writing tensor blk.18.attn_norm.weight                | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[ 93/291] Writing tensor blk.18.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   2\n",
      "INFO:convert:[ 94/291] Writing tensor blk.18.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 95/291] Writing tensor blk.18.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 96/291] Writing tensor blk.18.ffn_norm.weight                 | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[ 97/291] Writing tensor blk.18.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 98/291] Writing tensor blk.18.attn_output.weight              | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[ 99/291] Writing tensor blk.18.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[100/291] Writing tensor blk.18.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[101/291] Writing tensor blk.19.attn_norm.weight                | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[102/291] Writing tensor blk.19.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   2\n",
      "INFO:convert:[103/291] Writing tensor blk.19.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[104/291] Writing tensor blk.19.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[105/291] Writing tensor blk.19.ffn_norm.weight                 | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[106/291] Writing tensor blk.19.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[107/291] Writing tensor blk.19.attn_output.weight              | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[108/291] Writing tensor blk.19.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[109/291] Writing tensor blk.19.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[110/291] Writing tensor blk.2.attn_norm.weight                 | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[111/291] Writing tensor blk.2.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   2\n",
      "INFO:convert:[112/291] Writing tensor blk.2.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[113/291] Writing tensor blk.2.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[114/291] Writing tensor blk.2.ffn_norm.weight                  | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[115/291] Writing tensor blk.2.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[116/291] Writing tensor blk.2.attn_output.weight               | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[117/291] Writing tensor blk.2.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[118/291] Writing tensor blk.2.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[119/291] Writing tensor blk.20.attn_norm.weight                | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[120/291] Writing tensor blk.20.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   2\n",
      "INFO:convert:[121/291] Writing tensor blk.20.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[122/291] Writing tensor blk.20.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[123/291] Writing tensor blk.20.ffn_norm.weight                 | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[124/291] Writing tensor blk.20.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[125/291] Writing tensor blk.20.attn_output.weight              | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[126/291] Writing tensor blk.20.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[127/291] Writing tensor blk.20.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[128/291] Writing tensor blk.21.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[129/291] Writing tensor blk.21.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   2\n",
      "INFO:convert:[130/291] Writing tensor blk.3.attn_norm.weight                 | size   3072           | type F32  | T+   2\n",
      "INFO:convert:[131/291] Writing tensor blk.3.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   3\n",
      "INFO:convert:[132/291] Writing tensor blk.3.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[133/291] Writing tensor blk.3.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[134/291] Writing tensor blk.3.ffn_norm.weight                  | size   3072           | type F32  | T+   3\n",
      "INFO:convert:[135/291] Writing tensor blk.3.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[136/291] Writing tensor blk.3.attn_output.weight               | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[137/291] Writing tensor blk.3.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[138/291] Writing tensor blk.3.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[139/291] Writing tensor blk.4.attn_norm.weight                 | size   3072           | type F32  | T+   3\n",
      "INFO:convert:[140/291] Writing tensor blk.4.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   3\n",
      "INFO:convert:[141/291] Writing tensor blk.4.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[142/291] Writing tensor blk.4.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[143/291] Writing tensor blk.4.ffn_norm.weight                  | size   3072           | type F32  | T+   3\n",
      "INFO:convert:[144/291] Writing tensor blk.4.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[145/291] Writing tensor blk.4.attn_output.weight               | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[146/291] Writing tensor blk.4.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[147/291] Writing tensor blk.4.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[148/291] Writing tensor blk.5.attn_norm.weight                 | size   3072           | type F32  | T+   3\n",
      "INFO:convert:[149/291] Writing tensor blk.5.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   3\n",
      "INFO:convert:[150/291] Writing tensor blk.5.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[151/291] Writing tensor blk.5.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[152/291] Writing tensor blk.5.ffn_norm.weight                  | size   3072           | type F32  | T+   3\n",
      "INFO:convert:[153/291] Writing tensor blk.5.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[154/291] Writing tensor blk.5.attn_output.weight               | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[155/291] Writing tensor blk.5.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[156/291] Writing tensor blk.5.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[157/291] Writing tensor blk.6.attn_norm.weight                 | size   3072           | type F32  | T+   3\n",
      "INFO:convert:[158/291] Writing tensor blk.6.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   3\n",
      "INFO:convert:[159/291] Writing tensor blk.6.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[160/291] Writing tensor blk.6.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[161/291] Writing tensor blk.6.ffn_norm.weight                  | size   3072           | type F32  | T+   3\n",
      "INFO:convert:[162/291] Writing tensor blk.6.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[163/291] Writing tensor blk.6.attn_output.weight               | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[164/291] Writing tensor blk.6.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[165/291] Writing tensor blk.6.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   3\n",
      "INFO:convert:[166/291] Writing tensor blk.7.attn_norm.weight                 | size   3072           | type F32  | T+   3\n",
      "INFO:convert:[167/291] Writing tensor blk.7.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   4\n",
      "INFO:convert:[168/291] Writing tensor blk.7.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[169/291] Writing tensor blk.7.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[170/291] Writing tensor blk.7.ffn_norm.weight                  | size   3072           | type F32  | T+   4\n",
      "INFO:convert:[171/291] Writing tensor blk.7.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[172/291] Writing tensor blk.7.attn_output.weight               | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[173/291] Writing tensor blk.7.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[174/291] Writing tensor blk.7.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[175/291] Writing tensor blk.8.attn_norm.weight                 | size   3072           | type F32  | T+   4\n",
      "INFO:convert:[176/291] Writing tensor blk.8.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   4\n",
      "INFO:convert:[177/291] Writing tensor blk.8.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[178/291] Writing tensor blk.8.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[179/291] Writing tensor blk.8.ffn_norm.weight                  | size   3072           | type F32  | T+   4\n",
      "INFO:convert:[180/291] Writing tensor blk.8.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[181/291] Writing tensor blk.8.attn_output.weight               | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[182/291] Writing tensor blk.8.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[183/291] Writing tensor blk.8.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[184/291] Writing tensor blk.9.attn_norm.weight                 | size   3072           | type F32  | T+   4\n",
      "INFO:convert:[185/291] Writing tensor blk.9.ffn_down.weight                  | size   3072 x   8192  | type F16  | T+   4\n",
      "INFO:convert:[186/291] Writing tensor blk.9.ffn_gate.weight                  | size   8192 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[187/291] Writing tensor blk.9.ffn_up.weight                    | size   8192 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[188/291] Writing tensor blk.9.ffn_norm.weight                  | size   3072           | type F32  | T+   4\n",
      "INFO:convert:[189/291] Writing tensor blk.9.attn_k.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[190/291] Writing tensor blk.9.attn_output.weight               | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[191/291] Writing tensor blk.9.attn_q.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[192/291] Writing tensor blk.9.attn_v.weight                    | size   3072 x   3072  | type F16  | T+   4\n",
      "INFO:convert:[193/291] Writing tensor output.weight                          | size  32064 x   3072  | type F16  | T+   5\n",
      "INFO:convert:[194/291] Writing tensor blk.21.attn_norm.weight                | size   3072           | type F32  | T+   5\n",
      "INFO:convert:[195/291] Writing tensor blk.21.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   5\n",
      "INFO:convert:[196/291] Writing tensor blk.21.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   5\n",
      "INFO:convert:[197/291] Writing tensor blk.21.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   5\n",
      "INFO:convert:[198/291] Writing tensor blk.21.ffn_norm.weight                 | size   3072           | type F32  | T+   5\n",
      "INFO:convert:[199/291] Writing tensor blk.21.attn_output.weight              | size   3072 x   3072  | type F16  | T+   5\n",
      "INFO:convert:[200/291] Writing tensor blk.21.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   5\n",
      "INFO:convert:[201/291] Writing tensor blk.22.attn_norm.weight                | size   3072           | type F32  | T+   5\n",
      "INFO:convert:[202/291] Writing tensor blk.22.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   5\n",
      "INFO:convert:[203/291] Writing tensor blk.22.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   5\n",
      "INFO:convert:[204/291] Writing tensor blk.22.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[205/291] Writing tensor blk.22.ffn_norm.weight                 | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[206/291] Writing tensor blk.22.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[207/291] Writing tensor blk.22.attn_output.weight              | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[208/291] Writing tensor blk.22.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[209/291] Writing tensor blk.22.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[210/291] Writing tensor blk.23.attn_norm.weight                | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[211/291] Writing tensor blk.23.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   6\n",
      "INFO:convert:[212/291] Writing tensor blk.23.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[213/291] Writing tensor blk.23.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[215/291] Writing tensor blk.23.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[216/291] Writing tensor blk.23.attn_output.weight              | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[217/291] Writing tensor blk.23.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[218/291] Writing tensor blk.23.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[219/291] Writing tensor blk.24.attn_norm.weight                | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[220/291] Writing tensor blk.24.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   6\n",
      "INFO:convert:[221/291] Writing tensor blk.24.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[222/291] Writing tensor blk.24.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[224/291] Writing tensor blk.24.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[225/291] Writing tensor blk.24.attn_output.weight              | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[226/291] Writing tensor blk.24.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[227/291] Writing tensor blk.24.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[228/291] Writing tensor blk.25.attn_norm.weight                | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[229/291] Writing tensor blk.25.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   6\n",
      "INFO:convert:[230/291] Writing tensor blk.25.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[231/291] Writing tensor blk.25.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[233/291] Writing tensor blk.25.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[234/291] Writing tensor blk.25.attn_output.weight              | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[235/291] Writing tensor blk.25.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[236/291] Writing tensor blk.25.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[237/291] Writing tensor blk.26.attn_norm.weight                | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[238/291] Writing tensor blk.26.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   6\n",
      "INFO:convert:[239/291] Writing tensor blk.26.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[240/291] Writing tensor blk.26.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[242/291] Writing tensor blk.26.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[243/291] Writing tensor blk.26.attn_output.weight              | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[244/291] Writing tensor blk.26.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[245/291] Writing tensor blk.26.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   6\n",
      "INFO:convert:[246/291] Writing tensor blk.27.attn_norm.weight                | size   3072           | type F32  | T+   6\n",
      "INFO:convert:[247/291] Writing tensor blk.27.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   7\n",
      "INFO:convert:[248/291] Writing tensor blk.27.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[249/291] Writing tensor blk.27.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   3072           | type F32  | T+   7\n",
      "INFO:convert:[251/291] Writing tensor blk.27.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[252/291] Writing tensor blk.27.attn_output.weight              | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[253/291] Writing tensor blk.27.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[254/291] Writing tensor blk.27.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[255/291] Writing tensor blk.28.attn_norm.weight                | size   3072           | type F32  | T+   7\n",
      "INFO:convert:[256/291] Writing tensor blk.28.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   7\n",
      "INFO:convert:[257/291] Writing tensor blk.28.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[258/291] Writing tensor blk.28.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   3072           | type F32  | T+   7\n",
      "INFO:convert:[260/291] Writing tensor blk.28.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[261/291] Writing tensor blk.28.attn_output.weight              | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[262/291] Writing tensor blk.28.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[263/291] Writing tensor blk.28.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[264/291] Writing tensor blk.29.attn_norm.weight                | size   3072           | type F32  | T+   7\n",
      "INFO:convert:[265/291] Writing tensor blk.29.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   7\n",
      "INFO:convert:[266/291] Writing tensor blk.29.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[267/291] Writing tensor blk.29.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   3072           | type F32  | T+   7\n",
      "INFO:convert:[269/291] Writing tensor blk.29.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[270/291] Writing tensor blk.29.attn_output.weight              | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[271/291] Writing tensor blk.29.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[272/291] Writing tensor blk.29.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[273/291] Writing tensor blk.30.attn_norm.weight                | size   3072           | type F32  | T+   7\n",
      "INFO:convert:[274/291] Writing tensor blk.30.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   7\n",
      "INFO:convert:[275/291] Writing tensor blk.30.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[276/291] Writing tensor blk.30.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   3072           | type F32  | T+   7\n",
      "INFO:convert:[278/291] Writing tensor blk.30.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[279/291] Writing tensor blk.30.attn_output.weight              | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[280/291] Writing tensor blk.30.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[281/291] Writing tensor blk.30.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   7\n",
      "INFO:convert:[282/291] Writing tensor blk.31.attn_norm.weight                | size   3072           | type F32  | T+   7\n",
      "INFO:convert:[283/291] Writing tensor blk.31.ffn_down.weight                 | size   3072 x   8192  | type F16  | T+   7\n",
      "INFO:convert:[284/291] Writing tensor blk.31.ffn_gate.weight                 | size   8192 x   3072  | type F16  | T+   8\n",
      "INFO:convert:[285/291] Writing tensor blk.31.ffn_up.weight                   | size   8192 x   3072  | type F16  | T+   8\n",
      "INFO:convert:[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   3072           | type F32  | T+   8\n",
      "INFO:convert:[287/291] Writing tensor blk.31.attn_k.weight                   | size   3072 x   3072  | type F16  | T+   8\n",
      "INFO:convert:[288/291] Writing tensor blk.31.attn_output.weight              | size   3072 x   3072  | type F16  | T+   8\n",
      "INFO:convert:[289/291] Writing tensor blk.31.attn_q.weight                   | size   3072 x   3072  | type F16  | T+   8\n",
      "INFO:convert:[290/291] Writing tensor blk.31.attn_v.weight                   | size   3072 x   3072  | type F16  | T+   8\n",
      "INFO:convert:[291/291] Writing tensor output_norm.weight                     | size   3072           | type F32  | T+   8\n",
      "INFO:convert:Wrote Phi-3-mini-4k-instruct-humor-full-clf-gguf-unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: ./Phi-3-mini-4k-instruct-humor-full-clf-gguf-unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 3012 (10b1e458)\n",
      "main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './Phi-3-mini-4k-instruct-humor-full-clf-gguf-unsloth.F16.gguf' to './Phi-3-mini-4k-instruct-humor-full-clf-gguf-unsloth.Q4_K_M.gguf' as Q4_K_M using 32 threads\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./Phi-3-mini-4k-instruct-humor-full-clf-gguf-unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi-3-mini-4k-instruct-humor-full-clf...\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32009\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "[   1/ 291]                    token_embd.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q4_K .. size =   187.88 MiB ->    52.84 MiB\n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  20/ 291]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 291]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  22/ 291]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  23/ 291]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  24/ 291]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  25/ 291]                 blk.10.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  26/ 291]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  27/ 291]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  28/ 291]                 blk.10.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  29/ 291]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 291]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  31/ 291]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  32/ 291]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  33/ 291]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  34/ 291]                 blk.11.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  35/ 291]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  36/ 291]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  37/ 291]                 blk.11.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  38/ 291]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 291]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  40/ 291]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  41/ 291]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  42/ 291]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  43/ 291]                 blk.12.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  44/ 291]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  45/ 291]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  46/ 291]                 blk.12.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  47/ 291]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 291]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  49/ 291]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  50/ 291]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  51/ 291]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  52/ 291]                 blk.13.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  53/ 291]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  54/ 291]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  55/ 291]                 blk.13.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  56/ 291]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 291]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  58/ 291]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  59/ 291]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  60/ 291]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  61/ 291]                 blk.14.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  62/ 291]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  63/ 291]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  64/ 291]                 blk.14.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  65/ 291]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 291]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  67/ 291]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  68/ 291]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  69/ 291]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  70/ 291]                 blk.15.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  71/ 291]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  72/ 291]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  73/ 291]                 blk.15.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  74/ 291]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 291]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  76/ 291]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  77/ 291]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  78/ 291]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  79/ 291]                 blk.16.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  80/ 291]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  81/ 291]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  82/ 291]                 blk.16.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  83/ 291]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 291]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  85/ 291]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  86/ 291]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  87/ 291]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  88/ 291]                 blk.17.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  89/ 291]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  90/ 291]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  91/ 291]                 blk.17.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  92/ 291]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 291]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  94/ 291]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  95/ 291]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  96/ 291]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  97/ 291]                 blk.18.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  98/ 291]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  99/ 291]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 100/ 291]                 blk.18.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 101/ 291]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 291]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 103/ 291]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 104/ 291]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 105/ 291]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 106/ 291]                 blk.19.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 107/ 291]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 108/ 291]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 109/ 291]                 blk.19.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 110/ 291]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 291]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 112/ 291]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 113/ 291]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 114/ 291]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 115/ 291]                  blk.2.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 116/ 291]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 117/ 291]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 118/ 291]                  blk.2.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 119/ 291]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 291]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 121/ 291]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 122/ 291]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 123/ 291]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 124/ 291]                 blk.20.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 125/ 291]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 126/ 291]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 127/ 291]                 blk.20.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 128/ 291]                 blk.21.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 129/ 291]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 130/ 291]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 131/ 291]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 132/ 291]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 133/ 291]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 134/ 291]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 135/ 291]                  blk.3.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 136/ 291]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 137/ 291]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 138/ 291]                  blk.3.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 139/ 291]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 140/ 291]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 141/ 291]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 142/ 291]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 143/ 291]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 144/ 291]                  blk.4.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 145/ 291]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 146/ 291]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 147/ 291]                  blk.4.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 148/ 291]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 149/ 291]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 150/ 291]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 151/ 291]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 152/ 291]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 153/ 291]                  blk.5.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 154/ 291]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 155/ 291]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 156/ 291]                  blk.5.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 157/ 291]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 158/ 291]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 159/ 291]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 160/ 291]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 161/ 291]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 162/ 291]                  blk.6.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 163/ 291]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 164/ 291]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 165/ 291]                  blk.6.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 166/ 291]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 167/ 291]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 168/ 291]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 169/ 291]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 170/ 291]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 171/ 291]                  blk.7.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 172/ 291]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 173/ 291]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 174/ 291]                  blk.7.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 175/ 291]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 176/ 291]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 177/ 291]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 178/ 291]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 179/ 291]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 180/ 291]                  blk.8.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 181/ 291]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 182/ 291]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 183/ 291]                  blk.8.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 184/ 291]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 185/ 291]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 186/ 291]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 187/ 291]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 188/ 291]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 189/ 291]                  blk.9.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 190/ 291]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 191/ 291]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 192/ 291]                  blk.9.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 193/ 291]                        output.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q6_K .. size =   187.88 MiB ->    77.06 MiB\n",
      "[ 194/ 291]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 291]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 196/ 291]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 197/ 291]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 198/ 291]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 199/ 291]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 200/ 291]                 blk.21.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 201/ 291]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 202/ 291]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 203/ 291]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 204/ 291]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 205/ 291]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 206/ 291]                 blk.22.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 207/ 291]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 208/ 291]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 209/ 291]                 blk.22.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 210/ 291]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 211/ 291]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 212/ 291]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 213/ 291]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 214/ 291]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 215/ 291]                 blk.23.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 216/ 291]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 217/ 291]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 218/ 291]                 blk.23.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 219/ 291]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 220/ 291]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 221/ 291]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 222/ 291]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 223/ 291]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 224/ 291]                 blk.24.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 225/ 291]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 226/ 291]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 227/ 291]                 blk.24.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 228/ 291]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 229/ 291]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 230/ 291]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 231/ 291]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 232/ 291]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 233/ 291]                 blk.25.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 234/ 291]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 235/ 291]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 236/ 291]                 blk.25.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 237/ 291]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 238/ 291]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 239/ 291]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 240/ 291]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 241/ 291]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 242/ 291]                 blk.26.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 243/ 291]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 244/ 291]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 245/ 291]                 blk.26.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 246/ 291]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 247/ 291]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 248/ 291]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 249/ 291]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 250/ 291]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 251/ 291]                 blk.27.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 252/ 291]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 253/ 291]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 254/ 291]                 blk.27.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 255/ 291]              blk.28.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 256/ 291]               blk.28.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 257/ 291]               blk.28.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 258/ 291]                 blk.28.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 259/ 291]               blk.28.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 260/ 291]                 blk.28.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 261/ 291]            blk.28.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 262/ 291]                 blk.28.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 263/ 291]                 blk.28.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 264/ 291]              blk.29.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 265/ 291]               blk.29.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 266/ 291]               blk.29.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 267/ 291]                 blk.29.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 268/ 291]               blk.29.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 269/ 291]                 blk.29.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 270/ 291]            blk.29.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 271/ 291]                 blk.29.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 272/ 291]                 blk.29.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 273/ 291]              blk.30.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 274/ 291]               blk.30.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 275/ 291]               blk.30.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 276/ 291]                 blk.30.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 277/ 291]               blk.30.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 278/ 291]                 blk.30.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 279/ 291]            blk.30.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 280/ 291]                 blk.30.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 281/ 291]                 blk.30.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 282/ 291]              blk.31.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 283/ 291]               blk.31.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 284/ 291]               blk.31.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 285/ 291]                 blk.31.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 286/ 291]               blk.31.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 287/ 291]                 blk.31.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 288/ 291]            blk.31.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 289/ 291]                 blk.31.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 290/ 291]                 blk.31.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 291/ 291]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "llama_model_quantize_internal: model size  =  7288.51 MB\n",
      "llama_model_quantize_internal: quant size  =  2210.78 MB\n",
      "\n",
      "main: quantize time = 41597.16 ms\n",
      "main:    total time = 41597.16 ms\n",
      "Unsloth: Conversion completed! Output location: ./Phi-3-mini-4k-instruct-humor-full-clf-gguf-unsloth.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"lora_model_full\")\n",
    "model.save_pretrained_gguf(\"Phi-3-mini-4k-instruct-humor-full-clf-gguf\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32009"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get pad t\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model_json_2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"./models/checkpoint-1000\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fd6b119bb7499287f68b7ca1bb41f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the test dataset\n",
    "from datasets import Dataset\n",
    "# Set up test prompt format\n",
    "test[\"conversations\"] = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a joke evaluator that answers in JSON. Here's the json schema you must adhere to:\\n{schema}\",\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": f\"\"\" Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"{joke}\" \"\"\"},\n",
    "    ] for joke in test[\"text\"]\n",
    "]\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "# Format for generations:\n",
    "def formatting_prompts_func_gen(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\") for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# Format the prompts\n",
    "test_dataset = test_dataset.map(formatting_prompts_func_gen, batched=True)\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "# Evaluate the model\n",
    "def evaluate (model, test_dataset): \n",
    "    preds = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs = torch.tensor(test_dataset[i][\"text\"]).to('cuda')\n",
    "        outputs = model.generate(inputs, max_length = 512)\n",
    "        preds.append(tokenizer.decode(outputs[0], skip_special_tokens = True))\n",
    "    return preds\n",
    "\n",
    "# Get the predictions\n",
    "# time the evaluation\n",
    "preds = evaluate(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels)):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels[i] \u001b[38;5;241m==\u001b[39m \u001b[43mpreds_clean\u001b[49m[i]:\n\u001b[1;32m      6\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# Get labels from the test set and compear to the predictions\n",
    "labels = test_dataset[\"label\"]\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == preds_clean[i]:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"Blonde and Barn What do a blonde and a barn have in common? They always have a cock in them.\"  \\n<|im_start|>assistant\\n{\"rating\": 0}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"great thyme The recipe says that I should put thyme exactly 10 seconds after turning the stove on. I guess the recipe needed great thyming.\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"What building has the most flights? The bar!\"  \\n<|im_start|>assistant\\n{\"rating\": 0}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"You guys hear about the fire at the circus? It was in tents....\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"The other day I called tech support... I asked if they knew how I could disable autocorrect on my mother-in-law.\"  \\n<|im_start|>assistant\\n{\"rating\": 4}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"Young mother with her boy So he\\'s sitting on the underground in London, and there\\'s a young mother with her son. He\\'s around 4 or 5. Apparently this kid is being kind of annoying, making noise, running around etc. Eventually the mother loses her cool and slaps the boy on the arm quite loudly. There\\'s a moment of silence, most of the people on the carriage were watching this kid, so they all saw her do it. For a second it looks like the kid is about to start bawling, but instead he turns, looks his mother straight in the eye and says; \"Are you proud of yourself?\"\"  \\n<|im_start|>assistant\\n{\"rating\": 4}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"A Mathematician, A Physicist and A Chemist were on a Beach They decided to put their expertise to use and conduct some research. The Math man said, \"I\\'ll jump into the water and measure the depth of the ocean.\" The Physicist said, \"I will go and examine the density of the water at various depths.\" The Chemist said, \"I will use the data you both collect and make some discovery about the chemical properties of the water. One after the other, the Mathematician and the Physicist jumped into the water. When they didn\\'t come out for a while, a bystander approached the Chemist regarding what was going on and why he wasn\\'t doing anything as his friends hadn\\'t come out of the water yet. The Chemist replied, \"You see, my friend, we are scientists and we were conducting an experiment here. And from my observations, I have concluded that the reason my friends haven\\'t come out is, because they are Soluble in water.\"\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"Girlfriend caught me having sex last night I\\'m surprised she woke up, next time I\\'ll use more chloroform.\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"If a homeless person were to get his hands on an iPhone Would he be able to use the home button?\"  \\n<|im_start|>assistant\\n{\"rating\": 0}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"If I had a penny for every time Donald Trump said something stupid, I would have a small loan of a million dollars\"  \\n<|im_start|>assistant\\n{\"rating\": 4}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"What\\'s the difference between mechanical engineers and civil engineers? Mechanical engineers build weapons... Civil engineers build targets.\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"I got in trouble for masturbating in the showers Apparently it ruined the school trip to auschwitz\"  \\n<|im_start|>assistant\\n{\"rating\": 4}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"I was eating green onions when all of a sudden, I started rhyming everything that I was saying. It turns out, they were rap scallions.\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"Blonde at the Super Bowl A guy took his blonde girlfriend to the super bowl game. They had great seats right behind their teamâ€™s bench. After the game, he asked her how she liked it. â€œOh, I really liked it,â€ she replied, â€œespecially the tight pants and all the big muscles, but I just couldnâ€™t understand why they were killing each other over 25 cents.â€ Dumbfounded, her boyfriend asked, â€œWhat do you mean?â€ â€œWell, they flipped a coin, one team got it and then for the rest of the game, all they kept screaming was... â€˜Get the quarterback! Get the quarterback!â€™ Iâ€™m like helloooooo? Itâ€™s only 25 cents!â€\"  \\n<|im_start|>assistant\\n{\"rating\": 4}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"What do Bob Dylan and ISIS have in common? Everybody must get stoned.\"  \\n<|im_start|>assistant\\n{\"rating\": 0}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"I am the smartest and the dumbest, the most handsome and the most ugly one amongst all my friends. . . yes you guessed it ... I have no friends . Ba Dum Tshhhh !\"  \\n<|im_start|>assistant\\n{\"rating\": 0}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"If I ever need a heart transplant.. I\\'d want my ex\\'s. It\\'s never been used.\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"I work for the world\\'s biggest NanoTechnology company We\\'re not very good\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"Mobius strips aren\\'t fair They\\'re completely one-sided.\"  \\n<|im_start|>assistant\\n{\"rating\": 3}',\n",
       " '<|im_start|>system\\nYou are a joke evaluator that answers in JSON. Here\\'s the json schema you must adhere to:\\n{\\'type\\': \\'object\\', \\'properties\\': {\\'rating\\': {\\'type\\': \\'number\\', \\'minimum\\': 0, \\'maximum\\': 4, \\'description\\': \\'The rating of the joke, from 0 to 5.\\'}}} \\n<|im_start|>user\\n Your task is to evaluate jokes based on their funniness on a scale from 0 to 4, where 0 represents the least funny and 4 represents the most funny. Consider the humor, originality, and overall impact of the joke when making your assessment: \\n \"Andy is charged with violent attack Judge asks him: \"You are well known as a gentleman, why would you hit that woman in the train? Explain yourself!\" \"Your honor, I was travelling by train as normal. When the controllor came and asked for ride tickets, I gave him the ticket. The woman, on the other hand, opened her suitcase, there was smaller suitcase, even smaller suitcase. Out of the smallest suitcase, she took out a case, out of that case, a smaller case, there was an even smaller case. Inside the case, there was a bag, smaller bag inside, and finally her wallet. So she handed the ticket for control, thrn she put it back to the wallet, wallet into bag, bag into bigger bag, bigger bag to even bigg..\" \"Shut the fuck up or I swear I\\'m gonna hurt you!\" \"You see now, your honor?\"\"  \\n<|im_start|>assistant\\n{\"rating\": 0}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21342     {\"rating\": 0}\n",
      "4946      {\"rating\": 0}\n",
      "93564     {\"rating\": 1}\n",
      "180534    {\"rating\": 3}\n",
      "52918     {\"rating\": 0}\n",
      "Name: label, dtype: object\n",
      "['{\"rating\": 0}', '{\"rating\": 3}', '{\"rating\": 0}', '{\"rating\": 3}', '{\"rating\": 4}']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25558850364963503"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract the predictions\n",
    "preds_clean = [re.search(r'<\\|im_start\\|>assistant\\n(.*)', pred).group(1) for pred in preds]\n",
    "\n",
    "print(test[\"label\"][0:5])\n",
    "print(preds_clean[0:5])\n",
    "# Compute the accuracy\n",
    "accuracy_score(test[\"label\"], preds_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preds\n",
       "{\"rating\": 0}    4334\n",
       "{\"rating\": 3}    3302\n",
       "{\"rating\": 4}    1132\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_clean_df = pd.DataFrame(preds_clean, columns = [\"preds\"])\n",
    "\n",
    "preds_clean_df[\"preds\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'{\"rating\": 4}': 1765,\n",
       "         '{\"rating\": 2}': 1756,\n",
       "         '{\"rating\": 1}': 1752,\n",
       "         '{\"rating\": 3}': 1752,\n",
       "         '{\"rating\": 0}': 1743})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique values in list\n",
    "from collections import Counter\n",
    "\n",
    "Counter(test_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.256\n",
      "Accuracy for label {\"rating\": 3}: 0.402\n",
      "Accuracy for label {\"rating\": 2}: 0.000\n",
      "Accuracy for label {\"rating\": 1}: 0.000\n",
      "Accuracy for label {\"rating\": 4}: 0.220\n",
      "Accuracy for label {\"rating\": 0}: 0.659\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "{\"rating\": 0}       0.26      0.66      0.38      1743\n",
      "{\"rating\": 1}       0.00      0.00      0.00      1752\n",
      "{\"rating\": 2}       0.00      0.00      0.00      1756\n",
      "{\"rating\": 3}       0.21      0.40      0.28      1752\n",
      "{\"rating\": 4}       0.34      0.22      0.27      1765\n",
      "\n",
      "     accuracy                           0.26      8768\n",
      "    macro avg       0.16      0.26      0.18      8768\n",
      " weighted avg       0.16      0.26      0.18      8768\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(conf_matrix)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_clean\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     33\u001b[0m y_true_num \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(reverse_mapping\u001b[38;5;241m.\u001b[39mget)(y_true)  \u001b[38;5;66;03m# Convert back to numerical labels\u001b[39;00m\n\u001b[1;32m     34\u001b[0m y_pred_num \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(reverse_mapping\u001b[38;5;241m.\u001b[39mget)(y_pred)\n\u001b[0;32m---> 35\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(conf_matrix)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:326\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    232\u001b[0m     {\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    243\u001b[0m ):\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:104\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    107\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: unknown is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    mapping = {0: \"Not funny at all\", 1: \"Not funny\", 2: \"Funny\", 3: \"Very funny\", 4: \"Hilarious\"}\n",
    "    reverse_mapping = {v: k for k, v in mapping.items()}  # Reverse mapping for confusion matrix\n",
    "\n",
    "    # Ensure y_true is string labels\n",
    "    if isinstance(y_true[0], (int, np.integer)):\n",
    "        map_func = np.vectorize(mapping.get)\n",
    "        y_true = map_func(y_true)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    y_true_num = np.vectorize(reverse_mapping.get)(y_true)  # Convert back to numerical labels\n",
    "    y_pred_num = np.vectorize(reverse_mapping.get)(y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_num, y_pred=y_pred_num, labels=list(mapping.keys()))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "evaluate(test_dataset[\"label\"], preds_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
