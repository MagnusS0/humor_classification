{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv(\n",
    "    '../data/external/preprocessed.csv',\n",
    "    dtype_backend='pyarrow',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'], unit='s')\n",
    "data['score'] = data['score'].astype('int64[pyarrow]')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remve the rows with missing values\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check summary statistics\n",
    "data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove jokes before 2016\n",
    "data = data[data['date'] > '2016-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot((data['score']+1), bins=50, color='skyblue', kde=True, log_scale=True) # Log +1 transformation\n",
    "plt.title('Distribution of Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of scores (95th percentile)\n",
    "# Calculate the 95th percentile limit\n",
    "limit = data['score'].quantile(0.95)\n",
    "\n",
    "# Create a subset of your data up to the 95th percentile\n",
    "data_95 = data[data['score'] <= limit]\n",
    "\n",
    "# Plot the distribution of scores (95th percentile)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data_95['score'], bins=100, color='skyblue', kde=True)\n",
    "plt.title('Distribution of Scores (95th Percentile)')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# List of offensive words to filter out\n",
    "offensive_words = [\n",
    "    \"nigger\", \"kike\", \"faggot\", \"retard\", \"sperg\",\n",
    "    \"tranny\", \"trannie\", \"shemale\", \"shim\", \"sodomite\",\n",
    "    \"cunt\", \"whore\", \"dyke\",\n",
    "    \"spic\", \"chink\", \"gook\", \"wetback\", \"beaner\",\n",
    "    \"coon\", \"jigaboo\", \"porch monkey\", \"tar baby\",\n",
    "    \"raghead\", \"towelhead\",\n",
    "    \"fag\", \"homo\", \"queer\", \"lesbo\", \"pansy\",\n",
    "     \"cripple\", \"mongoloid\",\n",
    "    \"rape\", \"molest\", \"pedophile\", \"child molester\",\n",
    "    \"jihadi\", \"lardass\", \"anorexic\", \"bulimic\",\n",
    "    \"slave\", \"plantation\", \"massa\", \"lynch\",\n",
    "    \"gas chamber\", \"holocaust\", \"nazi\", \"hitler\",\n",
    "    \"kkk\", \"klan\", \"white supremacist\",\n",
    "    \"suicide\", \"kill yourself\", \"self-harm\", \"cutting\",\n",
    "    \"pro-anorexia\", \"pro-bulimia\", \"thinspo\", \"bonespo\",\n",
    "    \"schizo\", \"gimp\", \"invalid\", \"gay\",\n",
    "]\n",
    "\n",
    "def preprocess_jokes(df, joke_column='joke', llm=False):\n",
    "    \"\"\"\n",
    "    Preprocess the jokes in a DataFrame for simple machine learning models.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the jokes.\n",
    "        joke_column (str): The column name containing the jokes.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional 'cleaned_joke' column.\n",
    "        int: Count of offensive jokes removed.\n",
    "        int: Count of jokes removed due to insufficient length.\n",
    "    \"\"\"\n",
    "    \n",
    "    def clean_joke(joke):\n",
    "        \"\"\"\n",
    "        Clean an individual joke.\n",
    "        \n",
    "        Args:\n",
    "            joke (str): The joke to clean.\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned joke.\n",
    "        \"\"\"\n",
    "        # Lowercase the joke\n",
    "        joke = joke.lower()\n",
    "        \n",
    "        # Remove URLs and non-joke content\n",
    "        joke = re.sub(r'http\\S+', '', joke)  # Remove URLs\n",
    "        joke = re.sub(r'\\s+', ' ', joke).strip()  # Remove excessive whitespace and trim\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        joke = joke.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "        \n",
    "        return joke\n",
    "    \n",
    "    def is_offensive(joke):\n",
    "            \"\"\"\n",
    "            Check if the joke contains any offensive words.\n",
    "            \n",
    "            Args:\n",
    "                joke (str): The joke to check.\n",
    "                \n",
    "            Returns:\n",
    "                bool: True if the joke contains offensive words, False otherwise.\n",
    "            \"\"\"\n",
    "\n",
    "            joke_words = joke.split()\n",
    "            return any(word.lower() in offensive_words for word in joke_words)\n",
    "        \n",
    "    offensive_count = 0\n",
    "    length_count = 0\n",
    "    \n",
    "    def process_row(joke):\n",
    "        nonlocal offensive_count, length_count\n",
    "        \n",
    "        if not isinstance(joke, str):\n",
    "            return None\n",
    "        \n",
    "        cleaned = clean_joke(joke)\n",
    "        \n",
    "        if is_offensive(cleaned):\n",
    "            offensive_count += 1\n",
    "            return None\n",
    "        \n",
    "        if len(cleaned) <= 10:\n",
    "            length_count += 1\n",
    "            return None\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Skip lower case and clean jokes for LLM\n",
    "    def process_row_llm(joke):\n",
    "        nonlocal offensive_count, length_count\n",
    "        \n",
    "        joke = re.sub(r'\\s+', ' ', joke).strip()\n",
    "\n",
    "        if not isinstance(joke, str):\n",
    "            return None\n",
    "        \n",
    "        if is_offensive(joke):\n",
    "            offensive_count += 1\n",
    "            return None\n",
    "        \n",
    "        if len(joke) <= 10:\n",
    "            length_count += 1\n",
    "            return None\n",
    "        \n",
    "        return joke\n",
    "    \n",
    "    if llm:\n",
    "        df['cleaned_joke'] = Parallel(n_jobs=-1)(delayed(process_row_llm)(row) for row in df[joke_column])\n",
    "    else:\n",
    "        df['cleaned_joke'] = Parallel(n_jobs=-1)(delayed(process_row)(row) for row in df[joke_column])\n",
    "    \n",
    "    duplicate_count = df.duplicated(subset=['cleaned_joke']).sum()\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['cleaned_joke'])\n",
    "\n",
    "    # Drop rows with None in 'cleaned_joke' column\n",
    "    df = df.dropna(subset=['cleaned_joke'])\n",
    "    \n",
    "    return df, offensive_count, length_count, duplicate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_jokes, offensive_count, length_count, duplicate_count = preprocess_jokes(data, 'joke', llm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cleaned Jokes: {len(cleaned_jokes)}\")\n",
    "print(f\"Offensive Jokes Removed: {offensive_count}\")\n",
    "print(f\"Jokes Removed Due to Length: {length_count}\")\n",
    "print(f\"Duplicate Jokes Removed: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_jokes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NA scores with 0\n",
    "cleaned_jokes['score'] = cleaned_jokes['score'].astype('Int64[pyarrow]')\n",
    "cleaned_jokes['score'] = cleaned_jokes['score'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all jokes with 0 scores\n",
    "cleaned_jokes = cleaned_jokes[cleaned_jokes['score'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform the scores\n",
    "cleaned_jokes['score'] = np.log1p(cleaned_jokes['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = cleaned_jokes['score'].quantile(0.95)\n",
    "\n",
    "data_95 = cleaned_jokes[cleaned_jokes['score'] <= limit]\n",
    "\n",
    "# Plot the distribution of scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot((data_95['score']), bins=50, color='skyblue', kde=True)\n",
    "plt.title('Distribution of Scores (95th Percentile)')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of words in each joke\n",
    "from nltk.tokenize import word_tokenize\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Apply function in parallel\n",
    "cleaned_jokes['tokenized_joke'] = Parallel(n_jobs=-1)(delayed(word_tokenize)(joke) for joke in cleaned_jokes['cleaned_joke'])\n",
    "\n",
    "# Calculate the number of words in each joke\n",
    "cleaned_jokes['num_words'] = cleaned_jokes['tokenized_joke'].apply(len)\n",
    "\n",
    "# Get 95 percentile of the number of words\n",
    "limit = cleaned_jokes['num_words'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of joke lengths\n",
    "downsample_data_95 = cleaned_jokes[cleaned_jokes['num_words'] <= limit]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(downsample_data_95['num_words'], bins=50, color='skyblue', kde=True)\n",
    "plt.title('Distribution of Joke Lengths (95th Percentile)')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_below_quantile(df, column, quantile=0.95):\n",
    "    original_count = len(df)\n",
    "    filtered_df = df[df[column] <= df[column].quantile(quantile)]\n",
    "    removed_count = original_count - len(filtered_df)\n",
    "    print(f\"Removed {removed_count} rows based on {column}\")\n",
    "    return filtered_df\n",
    "\n",
    "cleaned_jokes = filter_below_quantile(cleaned_jokes, 'score')\n",
    "cleaned_jokes = filter_below_quantile(cleaned_jokes, 'num_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make five classes of scores based on quantiles\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "cleaned_jokes['score_class'] = pd.qcut(cleaned_jokes['score'], 5, duplicates='drop', labels=labels)\n",
    "\n",
    "cleaned_jokes['score_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def undersampling(df, target_column, sample_strategy='auto'):\n",
    "    \"\"\"\n",
    "    undersampling the dataset to balance the distribution of the target column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        target_column (str): The column to balance.\n",
    "        sample_strategy (str): The sampling strategy to use.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The downsampled dataframe.\n",
    "    \"\"\"\n",
    "    # Create the undersampler\n",
    "    undersampler = RandomUnderSampler(sampling_strategy=sample_strategy, random_state=42)\n",
    "\n",
    "    # Separate the features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Fit the data to the undersampler\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "    # Combine the features and target\n",
    "    df_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_jokes = undersampling(cleaned_jokes, 'score_class', sample_strategy='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_jokes['score_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_jokes = cleaned_jokes[['cleaned_joke', 'score_class']]\n",
    "cleaned_jokes= cleaned_jokes.rename(columns={'cleaned_joke': 'text', 'score_class': 'label'})\n",
    "cleaned_jokes.to_parquet('../data/interim/cleaned_jokes.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean for use in Pretraining\n",
    "pretraining_data = cleaned_jokes[['cleaned_joke', 'score_class']]\n",
    "pretraining_data = pretraining_data.rename(columns={'cleaned_joke': 'text', 'score_class': 'label'})\n",
    "pretraining_data.to_parquet('../data/processed/pretraining_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_data = pd.read_parquet('../data/processed/pretraining_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a joke from each class\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "for label in labels:\n",
    "    print(f\"Class {label}:\")\n",
    "    print(pretraining_data[pretraining_data['label'] == label].iloc[0]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humor_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
